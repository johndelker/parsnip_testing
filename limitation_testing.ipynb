{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParSNIP Limitation Testing\n",
    "\n",
    "This notebook is for testing the limitations of ParSNIP by modifying datasets, making predictions and classifications, and comparing the results to those of unmodified data sets. Currently, it tests how the accuracy of ParSNIP degrades as the number of observations decreases.\n",
    "\n",
    "Written by John Delker (jfla@uw.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.table import Table, vstack\n",
    "import time\n",
    "import parsnip\n",
    "import lcdata\n",
    "from ipywidgets import interact, interactive, interactive_output, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from collections import namedtuple\n",
    "from IPython.display import display\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "# Hide a few warnings that would otherwise fill the page\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"'verbose' argument is deprecated and will be removed in a future release of LightGBM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bands = [\"lsstu\", \"lsstg\", \"lsstr\", \"lssti\", \"lsstz\", \"lssty\"] # Names of the bands used in the dataset\n",
    "band_colors = [\"blue\", \"green\", \"red\", \"purple\", \"brown\", \"black\"] # Colors used for the above bands when plotting\n",
    "model = parsnip.load_model(\"plasticc\") # Options: \"ps1\", \"plasticc\" - If changed, the bands and below dataset will need to be changed.\n",
    "Curve = namedtuple('Curve', 'time flux results')\n",
    "\n",
    "parsnip_data = \"/epyc/data/parsnip_tests/\"\n",
    "\n",
    "# Path to the dataset you want to train the model on for classifications\n",
    "# plasticc_test_slim is the first 5000 entries of the full platicc dataset.\n",
    "# You can alternatively use any dataset or subset of data you choose.\n",
    "dataset_path = os.path.join(parsnip_data, \"data/plasticc_test_slim.h5\")\n",
    "\n",
    "# Path to predictions from the model\n",
    "predictions_path = os.path.join(parsnip_data, \"predictions/parsnip_predictions_plasticc_train_aug_100.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'plasticc_test_slim.h5' as PLAsTiCC dataset...\n",
      "Rejecting 0 non-supernova-like light curves.\n",
      "Dataset contains 5000 light curves.\n"
     ]
    }
   ],
   "source": [
    "# Prepares the dataset, removing any objects whose type ParSNIP can not explicitly classify\n",
    "# NOTE: If not using Plasticc, some of this may need tweaking\n",
    "plasticc_dataset = parsnip.load_dataset(dataset_path)\n",
    "plasticc_dataset = plasticc_dataset[~(plasticc_dataset.meta['type'] == 'CaRT')]\n",
    "plasticc_dataset = plasticc_dataset[~(plasticc_dataset.meta['type'] == 'ILOT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier with keys:\n",
      "    color\n",
      "    color_error\n",
      "    s1\n",
      "    s1_error\n",
      "    s2\n",
      "    s2_error\n",
      "    s3\n",
      "    s3_error\n",
      "    luminosity\n",
      "    luminosity_error\n",
      "    reference_time_error\n",
      "[100]\tvalid_0's multi_logloss: 0.495647\n",
      "[100]\tvalid_0's multi_logloss: 0.509514\n",
      "[100]\tvalid_0's multi_logloss: 0.536781\n",
      "[100]\tvalid_0's multi_logloss: 0.531477\n",
      "[100]\tvalid_0's multi_logloss: 0.469776\n",
      "[100]\tvalid_0's multi_logloss: 0.518461\n",
      "[100]\tvalid_0's multi_logloss: 0.532923\n",
      "[100]\tvalid_0's multi_logloss: 0.488277\n",
      "[100]\tvalid_0's multi_logloss: 0.491106\n",
      "[100]\tvalid_0's multi_logloss: 0.549328\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier\n",
    "classifier = parsnip.Classifier()\n",
    "training_predictions = Table.read(predictions_path)\n",
    "training_classifications = classifier.train(training_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_bands(dataset, bands):\n",
    "    '''For each sample in the dataset, removes all points that are not part of the given bands'''\n",
    "    \n",
    "    # Makes a copy of the dataset so that the original is left unmodified\n",
    "    modified_dataset = lcdata.Dataset(dataset.meta.copy(), dataset.light_curves.copy())\n",
    "    \n",
    "    # Group the light curves by band\n",
    "    for i in range(0, len(modified_dataset)):\n",
    "        light_curve = dataset.light_curves[i].group_by('band')\n",
    "        group_count = len(light_curve.groups.indices) - 1\n",
    "\n",
    "        # Mask out all points that are part of any band that is being removed for this test\n",
    "        mask = np.ones(len(light_curve), dtype=bool)\n",
    "        for n in range(0, group_count):\n",
    "            if light_curve.groups.keys[\"band\"][n] in bands: continue\n",
    "            mask[light_curve.groups.indices[n] : light_curve.groups.indices[n + 1]] = False\n",
    "            \n",
    "        # Apply the mask and return to the original sorting method to prevent possible issues\n",
    "        modified_dataset.light_curves[i] = light_curve[mask]\n",
    "        modified_dataset.light_curves[i].sort(\"time\")\n",
    "    \n",
    "    return modified_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_observation_count(dataset, max_observations_per_band):\n",
    "    '''Removes observations past the max number of observations specified.'''\n",
    "    \n",
    "    # Makes a copy of the dataset so that the original is left unmodified\n",
    "    modified_dataset = lcdata.Dataset(dataset.meta.copy(), dataset.light_curves.copy())\n",
    "    \n",
    "    # For each object in the dataset, limit the number of observations in each band\n",
    "    for i in range(0, len(modified_dataset)):\n",
    "\n",
    "        # Group the light curves by band so that each band can be manipulated individually\n",
    "        light_curve = dataset.light_curves[i].group_by('band')\n",
    "        group_count = len(light_curve.groups.indices) - 1\n",
    "\n",
    "        # Create a mask to remove points from each group/band\n",
    "        mask = np.ones(len(light_curve), dtype=bool)\n",
    "        for n in range(0, group_count):\n",
    "            lower_bound = light_curve.groups.indices[n]\n",
    "            upper_bound = light_curve.groups.indices[n + 1]\n",
    "\n",
    "            # Otherwise, only mask out the portion that would be above the point cutoff\n",
    "            # Note: A cutoff of \"None\" retains all original observations\n",
    "            if max_observations_per_band is not None and lower_bound + max_observations_per_band < upper_bound:\n",
    "                mask[lower_bound + max_observations_per_band : upper_bound] = False\n",
    "\n",
    "        # Apply the mask and return to the original sorting method to prevent possible issues\n",
    "        modified_dataset.light_curves[i] = light_curve[mask]\n",
    "        modified_dataset.light_curves[i].sort(\"time\")\n",
    "    \n",
    "    return modified_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying modified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limitation_test(sample_size, bands, observation_cutoffs = [], object_index = None):\n",
    "    '''\n",
    "    Takes a sample of the given dataset and returns the predictions and classifications for each sample\n",
    "    along with predictions and classifications made using degraded versions of those sample.\n",
    "    '''\n",
    "    \n",
    "    global model, plasticc_dataset\n",
    "    \n",
    "    # If only looking at one object, grab that part of the dataset; otherwise, grab the first X objects\n",
    "    if not(object_index is None): dataset = plasticc_dataset[object_index]\n",
    "    else: dataset = plasticc_dataset[0:sample_size]\n",
    "        \n",
    "    # Remove unused bands from the dataset\n",
    "    dataset = restrict_bands(dataset, bands)\n",
    "    \n",
    "    # Classify the dataset with points removed for each cutoff given, and once without points removed as a control\n",
    "    observation_cutoffs.insert(0, None)\n",
    "    classified_data = { \"used_bands\": bands, \"used_cutoffs\": observation_cutoffs }\n",
    "    for cutoff in observation_cutoffs:\n",
    "    \n",
    "        # Modifies the dataset with a cutoff for observations, then generates predictions and classifications\n",
    "        modified_dataset = restrict_observation_count(dataset, cutoff)\n",
    "        predictions = model.predict_dataset(modified_dataset)\n",
    "        classifications = classifier.classify(predictions)\n",
    "\n",
    "        # For every object in the dataset, store the classification and relevant info\n",
    "        for index in range(0, len(predictions)):\n",
    "            \n",
    "            # Stores on a per-object basis to easily compare how an individual object is affected by modifications\n",
    "            object_id = classifications[\"object_id\"][index]\n",
    "            object_info = classified_data.get(object_id)\n",
    "            if object_info is None: object_info = []\n",
    "            \n",
    "            # Find the classification considered most likely by the classifier\n",
    "            top_prediction = None\n",
    "            for c in classifications.colnames:\n",
    "                if c == \"object_id\": continue\n",
    "                if top_prediction == None or classifications[c][index] > classifications[top_prediction][index]:\n",
    "                    top_prediction = c\n",
    "            \n",
    "            # Uses ParSNIP to predict the full light curve based on modified samples\n",
    "            predicted_curve = model.predict_light_curve(modified_dataset.light_curves[index], False)\n",
    "            \n",
    "            # Collects all the most important info from dataset, predictions, and classifications into one place\n",
    "            object_info.append({\n",
    "                \"cutoff\": cutoff, \n",
    "                \"truth\": modified_dataset.meta['type'][index],\n",
    "                \"prediction\": top_prediction,\n",
    "                \"light_curve\": modified_dataset.light_curves[index], \n",
    "                \"predicted_curve\": Curve(predicted_curve[0], predicted_curve[1], predicted_curve[2]),\n",
    "                \"SNIa\": classifications[\"SNIa\"][index], \n",
    "                \"SNII\": classifications[\"SNII\"][index], \n",
    "                \"SLSN-I\": classifications[\"SLSN-I\"][index], \n",
    "                \"SNIa-91bg\": classifications[\"SNIa-91bg\"][index], \n",
    "                \"SNIax\": classifications[\"SNIax\"][index], \n",
    "                \"SNIbc\": classifications[\"SNIbc\"][index], \n",
    "                \"TDE\": classifications[\"TDE\"][index], \n",
    "                \"KN\": classifications[\"KN\"][index]\n",
    "            })\n",
    "            \n",
    "            classified_data.update({ object_id: object_info })\n",
    "        \n",
    "    return classified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(dataset, cutoffs, object_index, cutoff, used_bands, shown_bands, show_scatter):\n",
    "    '''\n",
    "    Predicts the light curves for an object, for each modification made to that object, and plots\n",
    "    those light curves with scatter, error bars, and the unmodified light curve for comparison.\n",
    "    '''\n",
    "    \n",
    "    global model\n",
    "    \n",
    "    if \"ALL\" in used_bands: used_bands = all_bands\n",
    "    if \"ALL\" in shown_bands: shown_bands = all_bands\n",
    "        \n",
    "    cutoff_value = cutoffs[cutoff - 1] if cutoff > 0 else None\n",
    "    used_cutoffs = [cutoff_value] if cutoff > 0 else []\n",
    "    cutoff_index = 1 if cutoff > 0 else 0\n",
    "        \n",
    "    # Make classifications for the given settings\n",
    "    data = limitation_test(1, used_bands, used_cutoffs, object_index)\n",
    "    object_id = list(data.keys())[2]\n",
    "    true_class = data[object_id][0]['truth']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (12, 5))\n",
    "        \n",
    "    primary_light_curve = data[object_id][0][\"light_curve\"]\n",
    "    primary_predicted_curve = data[object_id][0][\"predicted_curve\"]\n",
    "    start_time = np.min(primary_predicted_curve.time)\n",
    "    \n",
    "    # Determining the vertical axis limits to show as much as possible without losing any data\n",
    "    if show_scatter != \"off\": min_y = min(primary_light_curve[\"flux\"])\n",
    "    else: min_y = min(primary_predicted_curve.flux.flatten())\n",
    "    max_y = max(primary_predicted_curve.flux.flatten())\n",
    "    \n",
    "    for band in shown_bands:\n",
    "        if not(band in used_bands): continue\n",
    "        band_index = all_bands.index(band)\n",
    "        \n",
    "        # Plots the predicted light curve for the unmodified sample in the given band\n",
    "        ax.plot((primary_predicted_curve.time - np.min(start_time)), primary_predicted_curve.flux[0][band_index], \n",
    "                            label = f\"{len(primary_light_curve)} points (original)\", \n",
    "                            linestyle = \"--\", linewidth = 1, color = band_colors[band_index])\n",
    "        \n",
    "        light_curve = data[object_id][cutoff_index][\"light_curve\"]\n",
    "        light_curve_mask = light_curve[\"band\"] == band\n",
    "\n",
    "        # Plot the individual observations and their flux error in the given band\n",
    "        if show_scatter != \"off\":\n",
    "            error = light_curve[light_curve_mask]['fluxerr'] if show_scatter == \"with error\" else np.zeros(len(light_curve[light_curve_mask]['fluxerr']))\n",
    "            ax.errorbar(light_curve[light_curve_mask]['time'] - start_time, light_curve[light_curve_mask]['flux'], \n",
    "                                    yerr = error, fmt = '.', label = band, color = band_colors[band_index])\n",
    "\n",
    "        if cutoff_value != None:\n",
    "            predicted_curve = data[object_id][cutoff_index][\"predicted_curve\"]\n",
    "            ax.plot(predicted_curve.time - start_time, predicted_curve.flux[0][band_index], \n",
    "                                label = f\"{len(light_curve)} points\", linewidth = 1.5, \n",
    "                                color = band_colors[band_index])\n",
    "\n",
    "            max_y = max(max(predicted_curve.flux.flatten()), max_y)\n",
    "        \n",
    "    ax.set_ylim(top = max_y + 2, bottom = min_y - 2)\n",
    "\n",
    "    # What is the predicted class and it's associated probability?\n",
    "    predicted_class = data[object_id][cutoff_index]['prediction']\n",
    "    prediction_probability = data[object_id][cutoff_index][predicted_class]\n",
    "    title = f\"{(prediction_probability * 100):.2f}% {predicted_class}\"\n",
    "\n",
    "    # If the predicted class is incorrect, what is the predicted probability of the true class?\n",
    "    if predicted_class != true_class: \n",
    "        truth_probability = data[object_id][cutoff_index][true_class]\n",
    "        title = f\"{title} ({(truth_probability * 100):.2f}% {true_class})\"\n",
    "\n",
    "    # Make predicted and true class probabilities the title of each plot\n",
    "    ax.set_title(title)\n",
    "    fig.suptitle(object_id)\n",
    "    #plt.legend()\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interactable_curve(dataset, cutoffs):\n",
    "    ''' Creates an interactable plot that compares predicted light curves. '''\n",
    "    \n",
    "    object_index = widgets.BoundedIntText(min = 0, max = len(plasticc_dataset) - 1, step = 1, value = 0, description = \"Object Index:\", continuous_update = False)\n",
    "    cutoff = widgets.IntSlider(min = 0, max = len(cutoffs), step = 1, value = 0, description = \"Cutoff:\", continuous_update = False)\n",
    "    used_bands = widgets.SelectMultiple(options = [\"ALL\"] + all_bands, rows = 3, value = [\"ALL\"], description = \"Use Bands:\")\n",
    "    shown_bands = widgets.SelectMultiple(options = [\"ALL\"] + all_bands, rows = 3, value = [\"ALL\"], description = \"See Bands:\")\n",
    "    show_scatter = widgets.RadioButtons(options = ['with error', 'no error', 'off'], description = \"Scatter:\")\n",
    "\n",
    "    ui = widgets.HBox([\n",
    "        widgets.VBox([object_index, cutoff, show_scatter]), \n",
    "        widgets.VBox([used_bands, shown_bands])\n",
    "    ])\n",
    "    \n",
    "    output = widgets.interactive_output(\n",
    "        plot_curve, {\n",
    "            \"dataset\": fixed(dataset), \n",
    "            \"cutoffs\": fixed(cutoffs), \n",
    "            \"object_index\": object_index,\n",
    "            \"cutoff\": cutoff, \n",
    "            \"used_bands\": used_bands, \n",
    "            \"shown_bands\": shown_bands, \n",
    "            \"show_scatter\": show_scatter\n",
    "        });\n",
    "    \n",
    "    display(output, ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classifications(data):\n",
    "    ''' A debug function to quickly check what probabilities are being assigned to each class for each object and modification.'''\n",
    "    for object_id in data:\n",
    "        if object_id == \"used_bands\" or object_id == \"used_cutoffs\": continue\n",
    "        print(\"==============================\")\n",
    "        print(f\"OBJECT {object_id}, TRUTH - {data[object_id][0]['truth']}\")\n",
    "        print(\"==============================\")\n",
    "        for index in range(0, len(data[object_id])):\n",
    "            print(\"------------------------------\")\n",
    "            print(f\"{len(data[object_id][index]['light_curve'])} POINTS USED\")\n",
    "            print(\"------------------------------\")\n",
    "            for key in data[object_id][index]:\n",
    "                if key != \"cutoff\" and key != \"light_curve\" and key != \"truth\" and key != \"predicted_curve\": \n",
    "                    text = f\"{key}: {data[object_id][index][key]}\"\n",
    "                    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(data):\n",
    "    '''\n",
    "    ~~FUNCTION IS A WORK IN PROGRESS AND NOT VERY HELPFUL AT THE MOMENT~~\n",
    "    Prints a table that shows us how accurate ParSNIP's predictions are to true classifications.\n",
    "    Each column is the true type. Each row is a possible predicted type.\n",
    "    ParSNIP assigns each possible predicted type a percentage chance that it is the true type.\n",
    "    The value in this table is found by summing all those percentages, then dividing by the total number of\n",
    "    objects that are the true type of that column (the \"Count\"). \n",
    "    Values closer to 100 mean ParSNIP generally leans towards that type.\n",
    "    '''\n",
    "    \n",
    "    # Table structure - Plasticc dataset has no KN objects, but ParSNIP can still predict them.\n",
    "    table = pandas.DataFrame({\n",
    "        \"SNIa\": { \"SNIa\" : 0., \"SNII\": 0, \"SLSN-I\": 0, \"SNIa-91bg\": 0, \"SNIax\": 0, \"SNIbc\": 0, \"TDE\": 0., \"KN\": 0., \"Count\": 0 },\n",
    "        \"SNII\": { \"SNIa\" : 0., \"SNII\": 0, \"SLSN-I\": 0, \"SNIa-91bg\": 0, \"SNIax\": 0, \"SNIbc\": 0, \"TDE\": 0., \"KN\": 0., \"Count\": 0 },\n",
    "        \"SLSN-I\": { \"SNIa\" : 0., \"SNII\": 0, \"SLSN-I\": 0, \"SNIa-91bg\": 0, \"SNIax\": 0, \"SNIbc\": 0, \"TDE\": 0., \"KN\": 0., \"Count\": 0 },\n",
    "        \"SNIa-91bg\": { \"SNIa\" : 0., \"SNII\": 0, \"SLSN-I\": 0, \"SNIa-91bg\": 0, \"SNIax\": 0, \"SNIbc\": 0, \"TDE\": 0., \"KN\": 0., \"Count\": 0 },\n",
    "        \"SNIax\": { \"SNIa\" : 0., \"SNII\": 0, \"SLSN-I\": 0, \"SNIa-91bg\": 0, \"SNIax\": 0, \"SNIbc\": 0, \"TDE\": 0., \"KN\": 0., \"Count\": 0 },\n",
    "        \"SNIbc\": { \"SNIa\" : 0., \"SNII\": 0, \"SLSN-I\": 0, \"SNIa-91bg\": 0, \"SNIax\": 0, \"SNIbc\": 0, \"TDE\": 0., \"KN\": 0., \"Count\": 0 },\n",
    "        \"TDE\": { \"SNIa\" : 0., \"SNII\": 0, \"SLSN-I\": 0, \"SNIa-91bg\": 0, \"SNIax\": 0, \"SNIbc\": 0, \"TDE\": 0., \"KN\": 0., \"Count\": 0 },\n",
    "        #\"KN\": { \"SNIa\" : 0., \"SNII\": 0, \"SLSN-I\": 0, \"SNIa-91bg\": 0, \"SNIax\": 0, \"SNIbc\": 0, \"TDE\": 0., \"KN\": 0., \"Count\": 0 }\n",
    "    })\n",
    "    \n",
    "    # For each object, add its percentage chance for each type to their row\n",
    "    for object_id in data:\n",
    "        if object_id == \"used_bands\" or object_id == \"used_cutoffs\": continue\n",
    "        truth = data[object_id][0]['truth']\n",
    "        table.loc[\"Count\", truth] += 1\n",
    "        #for index in range(0, len(data[object_id])):\n",
    "        index = 0\n",
    "        for key in data[object_id][index]:\n",
    "            if not(key in [\"cutoff\", \"light_curve\", \"truth\", \"prediction\", \"predicted_curve\"]): \n",
    "                #if data[object_id][index][key] > 0.9:\n",
    "                    #print(f\"{key}, {truth} += {data[object_id][index][key]} \")\n",
    "                table.loc[key, truth] += data[object_id][index][key]\n",
    "          \n",
    "    # Go through each column of true values and turn them into a percentage of the total objects that are truly this type\n",
    "    for col in table:\n",
    "        for index, row in table.iterrows():\n",
    "            if index == \"Count\": continue\n",
    "            table.loc[index, col] = (table.loc[index, col] * 100) / table.loc[\"Count\", col]\n",
    "        \n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40139e11f7424682a1cfc097ea4341a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b5d0cc3e064585b8ad01f0d78cc482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(BoundedIntText(value=0, description='Object Index:', max=4965), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_interactable_curve(plasticc_dataset, [32, 16, 8, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a plot where x axis is the point cutoff, y axis is the classifications and it is chopped into grids. \n",
    "#       Each block should either have a value or be colored based on how likely that classification is for that cutoff\n",
    "#       And possibly plot a stacked version for every sample combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset: 100%|██████████| 4500/4500 [00:09<00:00, 470.37it/s]\n",
      "Preprocessing dataset: 100%|██████████| 4500/4500 [00:09<00:00, 464.65it/s]\n"
     ]
    }
   ],
   "source": [
    "test_classifications = limitation_test(4500, all_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  SNIa         SNII     SLSN-I  SNIa-91bg       SNIax  \\\n",
      "SNIa         37.339582     1.966538   1.986523   2.102705   19.539480   \n",
      "SNII         29.009998    42.498728  31.679757   6.264664   32.390906   \n",
      "SLSN-I        1.394237     1.025496  58.710483   2.899390    0.260742   \n",
      "SNIa-91bg     5.830061     1.419360   1.515628  78.447973   12.081801   \n",
      "SNIax         7.411273     5.372542   0.331822   3.041586   15.011631   \n",
      "SNIbc         3.610319     8.953219   4.771415   5.719727   14.299634   \n",
      "TDE          15.122160    37.847889   0.892675   0.253343    4.437745   \n",
      "KN            0.282370     0.916228   0.111698   1.270612    1.978060   \n",
      "Count      1777.000000  2226.000000  10.000000  48.000000  121.000000   \n",
      "\n",
      "                SNIbc        TDE  \n",
      "SNIa         3.217834   0.416387  \n",
      "SNII        26.744472   2.354541  \n",
      "SLSN-I       0.355136   0.343325  \n",
      "SNIa-91bg   20.336642   0.230993  \n",
      "SNIax        7.056954   0.447421  \n",
      "SNIbc       36.420253   1.608646  \n",
      "TDE          2.143752  42.599617  \n",
      "KN           3.724956  51.999070  \n",
      "Count      311.000000   7.000000  \n"
     ]
    }
   ],
   "source": [
    "print_accuracy(test_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "John Delker's Python 3.10",
   "language": "python",
   "name": "jfla_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
